{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharvapatil123/ML_Practice/blob/master/Regression/Multiple_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Linear Regression"
      ],
      "metadata": {
        "id": "6UWYaxgEoJeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory"
      ],
      "metadata": {
        "id": "eiwGjqquoNoK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOyMzswBpRG"
      },
      "source": [
        "Have multiple Features\n",
        "\n",
        "y = b0 + b1\\*x1 + b2\\*x2 + ... + bn*xn\n",
        "\n",
        "*We don't apply FEATURE SCALING in this, as the coeff compensate with the featues and so brings them in a range*\n",
        "\n",
        "**Assumptions of Linear Regression:**\n",
        "\n",
        "1.**Linearity**: There must be a linear relationship(straight-line) between the dependent and independent variables.\n",
        "\n",
        "2.**Homoscedasticity**: Since your regression model never exactly predicts your dependent variable in practice, you always have an error. Now you can plot your dependent variable on the x axis and the error on the y axis.\n",
        "\n",
        "3.Multivariate normality\n",
        "\n",
        "4.Independence of errors\n",
        "\n",
        "5.Lack of **multicollinearity**: In multicollinearity, two or more of the predictors correlate strongly with each other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSttf7YAN_Ya"
      },
      "source": [
        "**Categorical Variable**: Place 1's and 0's adding an extra column each time a new row value is encountered (**Dummy Variables**)\n",
        "\n",
        "Numerical Variable: Those with numerical values\n",
        "\n",
        "y = b0 + b1\\*x1 + b2\\*x2 + b3\\*x3 + b4\\*D1\n",
        "\n",
        "Here, b0 = California, and \n",
        "\n",
        "b4 = New York - California\n",
        "\n",
        "So, Number of Dummy variables formed \n",
        "\n",
        "= No of characteristics - 1 \n",
        "\n",
        "Else, it causes **Dummy variable Trap**, which induces multicollinearity, as both the dummy variables effect are strongly dependent on each other.\n",
        "Thus, we always omit one dummy variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA63dLpGGa9W"
      },
      "source": [
        "Statistical SIgnificance\n",
        "\n",
        "If it goes beyond a value, our assumption is wrong, so reject NULL hypothesis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJWWCaQkQZQH"
      },
      "source": [
        "5 methods of building models\n",
        "\n",
        "1. **All-in**: Use all the variables\n",
        "\n",
        "> Prior Knowledge\n",
        "\n",
        "> You have to\n",
        "\n",
        "> Preparing for backward elimination\n",
        "\n",
        "2. **Backward Elimination**:  FASTEST\n",
        "\n",
        "> Select a significance level to stay in the model(e.g. SL=0.05)\n",
        "\n",
        "> Fit the full model with all possible predictors\n",
        "\n",
        "> Consider the predictor with highest P-value. If P>SL, go to next step, else FINISH\n",
        "\n",
        "> Remove the predictor\n",
        "\n",
        "> Fit model without this variable*\n",
        "\n",
        "3. **Forward Selection**\n",
        "\n",
        "> Select a significance level to enter in the model(e.g. SL=0.05)\n",
        "\n",
        "> Fit all simple regression models y ~ xn(i.e. create different regression models for each independent-dependent variable pair). Select the one with lowest P-value\n",
        "\n",
        "> Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have.\n",
        "\n",
        "> Consider the predictor with lowest P-value. If P < SL, go to previous step, else FINISH and keep the previous model\n",
        "\n",
        "4. **Bidirectional Elimination**\n",
        "\n",
        "> Select a significance level to enter and to stay in the model e.g. SLENTER = 0.05, SLSTAY = 0.05\n",
        "\n",
        "> Perform the next step of Forward Selection(new variables must have P < SLENTER to enter)\n",
        "\n",
        "> Perform all steps of Backward Selection(old variables must have P > SLSTAY to stay). Then go to previos step\n",
        "\n",
        "> No new variables can enter and no old variables can exit.(Your model is ready)\n",
        "\n",
        "STEPWISE REGRESSION 2-4\n",
        "\n",
        "5. **All Possible Models:** Constuct All possible regression models with 2^N - 1 total combinations\n",
        "\n",
        "6. **Score Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "Fgq8zYWAoQ8u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yInfi4dzBpk7"
      },
      "source": [
        "# Importing libraries\n",
        "# -------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXUEAJccN0qb"
      },
      "source": [
        "dataset = pd.read_csv('50_Startups.csv')\n",
        "x = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-MlSn3KN0s6"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNodaCROLh1"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujy4NtbxPdIR"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(transformers=[(\"encoder\", OneHotEncoder(), [3])], remainder = 'passthrough')\n",
        "x = np.array(ct.fit_transform(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzSr2A1SPdKz"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYEh_fbJO8nV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjnc9xfO8tK"
      },
      "source": [
        "print((x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_wyxw_isd0d"
      },
      "source": [
        "Training the Multiple Linear Regression model on the Traiing Set\n",
        "\n",
        "> The Multiple Regression Class itself avoids The Dummy Variable Trap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvlRdmUfseDD",
        "outputId": "9324254c-6120-4627-f3a0-7da7fbe94f3b"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# The class takes care of everything, starting from selecting the best feature for backward elimination to avoiding Dummy variables trap\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw7mOGwxseK2",
        "outputId": "be7bdbb0-a356-4d47-ca5e-1758c766f89d"
      },
      "source": [
        "y_pred = regressor.predict(x_test)\n",
        "np.set_printoptions(precision=2)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))\n",
        "print(y_pred.reshape(len(y_pred), 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[103015.2  103282.38]\n",
            " [132582.28 144259.4 ]\n",
            " [132447.74 146121.95]\n",
            " [ 71976.1   77798.83]\n",
            " [178537.48 191050.39]\n",
            " [116161.24 105008.31]\n",
            " [ 67851.69  81229.06]\n",
            " [ 98791.73  97483.56]\n",
            " [113969.44 110352.25]\n",
            " [167921.07 166187.94]]\n",
            "[[103015.2 ]\n",
            " [132582.28]\n",
            " [132447.74]\n",
            " [ 71976.1 ]\n",
            " [178537.48]\n",
            " [116161.24]\n",
            " [ 67851.69]\n",
            " [ 98791.73]\n",
            " [113969.44]\n",
            " [167921.07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnZb4QLCxwgg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0abBl1vxwi2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}